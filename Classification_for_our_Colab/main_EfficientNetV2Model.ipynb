{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/dacon/lowresol/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -qn open.zip -d ./open/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet timm pytorch_lightning==1.7.7 torchmetrics==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as L\n",
    "\n",
    "from torchinfo import summary\n",
    "#from glob import glob\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as  transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Swinv2Config, Swinv2Model, AutoImageProcessor, AutoModelForImageClassification\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger  # wandb logger를 임포트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_DEVICES = torch.cuda.device_count()\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "    #NUM_CLASSES = 4\n",
    "    NUM_CLASSES = 8\n",
    "    EPOCHS = 16\n",
    "    BATCH_SIZE = (\n",
    "        32 if torch.cuda.device_count() < 2 \n",
    "        else (32 * torch.cuda.device_count())\n",
    "    )\n",
    "    LR = 0.001\n",
    "    APPLY_SHUFFLE = True\n",
    "    SEED = 768\n",
    "    #HEIGHT = 224\n",
    "    #WIDTH = 224\n",
    "    HEIGHT = 256\n",
    "    WIDTH = 256\n",
    "    CHANNELS = 3\n",
    "    #IMAGE_SIZE = (224, 224, 3)\n",
    "    IMAGE_SIZE = (256, 256, 3)\n",
    "    \n",
    "    # Define paths\n",
    "    #DATASET_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
    "    #TRAIN_PATH = '/content/drive/MyDrive/Colab Notebooks/dataset/train/'\n",
    "    #TEST_PATH = '/content/drive/MyDrive/Colab Notebooks/dataset/test'\n",
    "    \n",
    "    ## Define paths\n",
    "    #DATASET_PATH = \"C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Data\"\n",
    "    #TRAIN_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Data\\\\train\\\\'\n",
    "    #TEST_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Data\\\\test\\\\'\n",
    "\n",
    "    # Define paths\n",
    "    DATASET_PATH = \"C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Datas_3000_3000\"\n",
    "    TRAIN_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Datas_3000_3000\\\\'\n",
    "    TEST_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Datas_3000_3000_downsampling\\\\'\n",
    "    \n",
    "    \n",
    "# Mute warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'saved'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "result_dir = 'result'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### Get image paths with glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#print(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "train_images = glob.glob(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "valid_images = glob.glob(f\"{CFG.TEST_PATH}**/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### Create Pandas DataFrames for paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(image_paths):\n",
    "    return [(_.split('\\\\')[-2:][0]).replace('-', '_') for _ in image_paths]\n",
    "\n",
    "\n",
    "def build_df(image_paths, labels):\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'image_path': image_paths,\n",
    "        'label': generate_labels(labels)\n",
    "    })\n",
    "    \n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>aeroplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>aeroplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>aeroplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>aeroplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>aeroplane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path      label\n",
       "0  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  aeroplane\n",
       "1  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  aeroplane\n",
       "2  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  aeroplane\n",
       "3  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  aeroplane\n",
       "4  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  aeroplane"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the DataFrames\n",
    "train_df = build_df(train_images, train_images)\n",
    "test_df = build_df(valid_images, valid_images)\n",
    "\n",
    "# View first 5 samples in the dataset\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_kfold_df = build_df(train_images, train_images)\n",
    "valid_no_kfold_df = build_df(valid_images, valid_images)\n",
    "\n",
    "# View first 5 samples in the dataset\n",
    "train_no_kfold_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_no_kfold_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_kfold_df.shape, valid_no_kfold_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### Load & View Random Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load(image_path, as_tensor=True):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if as_tensor:\n",
    "        converter = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            #transforms.Grayscale()\n",
    "        ])\n",
    "        return converter(image)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "\n",
    "def view_sample(image, label, color_map='rgb', fig_size=(8, 10)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    \n",
    "    if color_map=='rgb':\n",
    "        plt.imshow(image)\n",
    "    else:\n",
    "        plt.imshow(image, cmap=color_map)\n",
    "    \n",
    "    plt.title(f'Label: {label}', fontsize=16)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'mako' is not a valid value for cmap; supported values are 'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Grays', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_grey', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gist_yerg', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'grey', 'hot', 'hot_r', 'hsv', 'hsv_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'winter', 'winter_r'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m sample_image, sample_label \u001b[38;5;241m=\u001b[39m _load(train_df\u001b[38;5;241m.\u001b[39mimage_path[idx]), train_df\u001b[38;5;241m.\u001b[39mlabel[idx]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# View the random sample\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mview_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmako\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m, in \u001b[0;36mview_sample\u001b[1;34m(image, label, color_map, fig_size)\u001b[0m\n\u001b[0;32m     19\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(image)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\pyplot.py:3358\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   3337\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   3338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   3339\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3356\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[1;32m-> 3358\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[0;32m   3359\u001b[0m         X,\n\u001b[0;32m   3360\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[0;32m   3361\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   3362\u001b[0m         aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[0;32m   3363\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m   3364\u001b[0m         alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[0;32m   3365\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[0;32m   3366\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[0;32m   3367\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m   3368\u001b[0m         extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[0;32m   3369\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   3370\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   3371\u001b[0m         filterrad\u001b[38;5;241m=\u001b[39mfilterrad,\n\u001b[0;32m   3372\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   3373\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   3374\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3375\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3376\u001b[0m     )\n\u001b[0;32m   3377\u001b[0m     sci(__ret)\n\u001b[0;32m   3378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(ax, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5745\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5540\u001b[0m \u001b[38;5;129m@_preprocess_data\u001b[39m()\n\u001b[0;32m   5541\u001b[0m \u001b[38;5;129m@_docstring\u001b[39m\u001b[38;5;241m.\u001b[39minterpd\n\u001b[0;32m   5542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5545\u001b[0m            interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[0;32m   5546\u001b[0m            resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   5547\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5548\u001b[0m \u001b[38;5;124;03m    Display data as an image, i.e., on a 2D regular raster.\u001b[39;00m\n\u001b[0;32m   5549\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5743\u001b[0m \u001b[38;5;124;03m    (unassociated) alpha representation.\u001b[39;00m\n\u001b[0;32m   5744\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5745\u001b[0m     im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m   5746\u001b[0m                           interpolation\u001b[38;5;241m=\u001b[39minterpolation, origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m   5747\u001b[0m                           extent\u001b[38;5;241m=\u001b[39mextent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m   5748\u001b[0m                           filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m   5749\u001b[0m                           interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m   5750\u001b[0m                           \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   5752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m   5753\u001b[0m             im\u001b[38;5;241m.\u001b[39mis_transform_set()\n\u001b[0;32m   5754\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_transform()\u001b[38;5;241m.\u001b[39mcontains_branch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransData)):\n\u001b[0;32m   5755\u001b[0m         aspect \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage.aspect\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\image.py:912\u001b[0m, in \u001b[0;36mAxesImage.__init__\u001b[1;34m(self, ax, cmap, norm, interpolation, origin, extent, filternorm, filterrad, resample, interpolation_stage, **kwargs)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ax,\n\u001b[0;32m    897\u001b[0m              \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    898\u001b[0m              cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    907\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    908\u001b[0m              ):\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extent \u001b[38;5;241m=\u001b[39m extent\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    913\u001b[0m         ax,\n\u001b[0;32m    914\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[0;32m    915\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[0;32m    916\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m    917\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[0;32m    918\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[0;32m    919\u001b[0m         filterrad\u001b[38;5;241m=\u001b[39mfilterrad,\n\u001b[0;32m    920\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m    921\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[0;32m    922\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    923\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\image.py:261\u001b[0m, in \u001b[0;36m_ImageBase.__init__\u001b[1;34m(self, ax, cmap, norm, interpolation, origin, filternorm, filterrad, resample, interpolation_stage, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ax,\n\u001b[0;32m    249\u001b[0m              cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    250\u001b[0m              norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    259\u001b[0m              ):\n\u001b[0;32m    260\u001b[0m     martist\u001b[38;5;241m.\u001b[39mArtist\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 261\u001b[0m     \u001b[43mcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScalarMappable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m         origin \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage.origin\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\cm.py:416\u001b[0m, in \u001b[0;36mScalarMappable.__init__\u001b[1;34m(self, norm, cmap)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_norm(norm)  \u001b[38;5;66;03m# The Normalize instance of this ScalarMappable.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# So that the setter knows we're initializing.\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_cmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# The Colormap instance of this ScalarMappable.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m#: The last colorbar associated with this ScalarMappable. May be None.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolorbar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\cm.py:605\u001b[0m, in \u001b[0;36mScalarMappable.set_cmap\u001b[1;34m(self, cmap)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mSet the colormap for luminance data.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;124;03mcmap : `.Colormap` or str or None\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    603\u001b[0m in_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcmap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcmap \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_cmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m in_init:\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchanged()\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\cm.py:744\u001b[0m, in \u001b[0;36m_ensure_cmap\u001b[1;34m(cmap)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;66;03m# use check_in_list to ensure type stability of the exception raised by\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;66;03m# the internal usage of this (ValueError vs KeyError)\u001b[39;00m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmap_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _colormaps:\n\u001b[1;32m--> 744\u001b[0m     \u001b[43m_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_in_list\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_colormaps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mcolormaps[cmap_name]\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\matplotlib\\_api\\__init__.py:129\u001b[0m, in \u001b[0;36mcheck_in_list\u001b[1;34m(values, _print_supported_values, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _print_supported_values:\n\u001b[0;32m    128\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; supported values are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mrepr\u001b[39m,\u001b[38;5;250m \u001b[39mvalues))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: 'mako' is not a valid value for cmap; supported values are 'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Grays', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_grey', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gist_yerg', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'grey', 'hot', 'hot_r', 'hsv', 'hsv_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'winter', 'winter_r'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select random sample from train_df\n",
    "idx = random.sample(train_df.index.to_list(), 1)[0]\n",
    "\n",
    "# Load the random sample and label\n",
    "sample_image, sample_label = _load(train_df.image_path[idx]), train_df.label[idx]\n",
    "\n",
    "# View the random sample\n",
    "view_sample(\n",
    "    sample_image.permute(1, 2, 0),\n",
    "    sample_label,\n",
    "    color_map='mako',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.5'></a>\n",
    "### Create Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train/Val split with Training Set\n",
    "train_split_idx, val_split_idx, _, _ = (\n",
    "    train_test_split(\n",
    "        train_df.index, \n",
    "        train_df.label, \n",
    "        test_size=0.20,\n",
    "        stratify=train_df.label,\n",
    "        random_state=CFG.SEED\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2440, 2), (611, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get training and remaining data\n",
    "train_new_df = train_df.iloc[train_split_idx].reset_index(drop=True)\n",
    "val_df = train_df.iloc[val_split_idx].reset_index(drop=True)\n",
    "\n",
    "# View shapes\n",
    "train_new_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, path_col,  mode='train'):\n",
    "        self.df = df\n",
    "        self.path_col = path_col\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            label = row['class']\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'label':label\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'val':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            label = row['class']\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'label':label\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'inference':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            data = {\n",
    "                'image':image,\n",
    "            }\n",
    "            return data\n",
    "\n",
    "    def train_transform(self, image):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollateFn:\n",
    "    def __init__(self, transform, mode):\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.mode=='train':\n",
    "            pixel_values = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            label = torch.LongTensor([data['label'] for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "                'label':label,\n",
    "            }\n",
    "        elif self.mode=='val':\n",
    "            pixel_values = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            label = torch.LongTensor([data['label'] for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "                'label':label,\n",
    "            }\n",
    "        elif self.mode=='inference':\n",
    "            pixel_values = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            return {\n",
    "                'pixel_values':pixel_values,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.LazyLinear(CFG.NUM_CLASSES),\n",
    "        )\n",
    "\n",
    "#     @torch.compile\n",
    "    def forward(self, x, label=None):\n",
    "        # original\n",
    "        # x = self.model(x).pooler_output\n",
    "        x = self.model(x)\n",
    "        # pooler_output 대신에 last_hidden_state 사용\n",
    "        #x = outputs.last_hidden_state[:, 0]  # [CLS] 토큰에 해당하는 벡터 추출\n",
    "        #x = self.clf(x)\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x, label)\n",
    "        probs = nn.LogSoftmax(dim=-1)(x)\n",
    "        return probs, loss\n",
    "\n",
    "class LitCustomModel(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = CustomModel(model)\n",
    "        self.validation_step_output = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        return opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        x = batch['pixel_values']\n",
    "        label = batch['label']\n",
    "        probs, loss = self.model(x, label)\n",
    "        self.log(f\"train_loss\", loss, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx=None):\n",
    "        x = batch['pixel_values']\n",
    "        label = batch['label']\n",
    "        probs, loss = self.model(x, label)\n",
    "        self.validation_step_output.append([probs,label])\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx=None):\n",
    "        x = batch['pixel_values']\n",
    "        probs, _ = self.model(x)\n",
    "        return probs\n",
    "\n",
    "    def validation_epoch_end(self, step_output):\n",
    "        pred = torch.cat([x for x, _ in self.validation_step_output]).cpu().detach().numpy().argmax(1)\n",
    "        label = torch.cat([label for _, label in self.validation_step_output]).cpu().detach().numpy()\n",
    "        score = f1_score(label,pred, average='macro')\n",
    "        self.log(\"val_score\", score)\n",
    "        self.validation_step_output.clear()\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_SPLIT = 5\n",
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_csv('./open/train.csv')\n",
    "train_df = train_new_df\n",
    "#train_df['img_path'] = train_df['img_path'].apply(lambda x: os.path.join('./open', x))\n",
    "#train_df['upscale_img_path'] = train_df['upscale_img_path'].apply(lambda x: os.path.join('./open', x))\n",
    "le = LabelEncoder()\n",
    "train_df['class'] = le.fit_transform(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_no_kfold = LabelEncoder()\n",
    "train_no_kfold_df['class'] = le_no_kfold.fit_transform(train_no_kfold_df['label'])\n",
    "valid_no_kfold_df['class'] = le_no_kfold.fit_transform(valid_no_kfold_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 예시 데이터프레임 생성\n",
    "# valid_no_kfold_df = pd.DataFrame({\n",
    "#     'label': [...],\n",
    "#     'class': [...]\n",
    "# })\n",
    "\n",
    "# 샘플링할 데이터 비율 (예: 전체 데이터의 20%)\n",
    "sample_ratio = 0.4\n",
    "\n",
    "# StratifiedShuffleSplit 객체 생성\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=sample_ratio, random_state=42)\n",
    "\n",
    "# 샘플링\n",
    "for train_index, sample_index in splitter.split(valid_no_kfold_df, valid_no_kfold_df['class']):\n",
    "    sampled_df = valid_no_kfold_df.iloc[sample_index]\n",
    "\n",
    "valid_no_kfold_df = sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not len(train_df) == len(os.listdir('./open/train')):\n",
    "#    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLIT, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256,256), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256,256), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "train_collate_fn = CustomCollateFn(train_transform, 'train')\n",
    "val_collate_fn = CustomCollateFn(val_transform, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Model(nn.Module):\n",
    "    def __init__(self, backbone_model, name='efficientnet-v2-large', \n",
    "                 num_classes=CFG.NUM_CLASSES, device=CFG.DEVICE):\n",
    "        super(EfficientNetV2Model, self).__init__()\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.name = name\n",
    "        \n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(in_features=1280, out_features=256, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=num_classes, bias=False)\n",
    "        ).to(device)\n",
    "        \n",
    "        self._set_classifier(classifier)\n",
    "        \n",
    "    def _set_classifier(self, classifier:nn.Module) -> None:\n",
    "        self.backbone_model.classifier = classifier\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.backbone_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effiecientnetv2_model(\n",
    "    device: torch.device=CFG.NUM_CLASSES) -> nn.Module:\n",
    "    # Set the manual seeds\n",
    "    torch.manual_seed(CFG.SEED)\n",
    "    torch.cuda.manual_seed(CFG.SEED)\n",
    "\n",
    "    # Get model weights\n",
    "    model_weights = (\n",
    "        torchvision\n",
    "        .models\n",
    "        .EfficientNet_V2_L_Weights\n",
    "        .DEFAULT\n",
    "    )\n",
    "    \n",
    "    # Get model and push to device\n",
    "    model = (\n",
    "        torchvision.models.efficientnet_v2_l(\n",
    "            weights=model_weights\n",
    "        )\n",
    "    ).to(device) \n",
    "    \n",
    "    # Freeze Model Parameters\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EfficientNet v2 model\n",
    "backbone_model = get_effiecientnetv2_model(CFG.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetv2_params = {\n",
    "    'backbone_model'    : backbone_model,\n",
    "    'name'              : 'efficientnet-v2-large',\n",
    "    'device'            : CFG.DEVICE\n",
    "}\n",
    "\n",
    "# Generate Model\n",
    "efficientnet_model = EfficientNetV2Model(**efficientnetv2_params)\n",
    "\n",
    "# If using GPU T4 x2 setup, use this:\n",
    "if CFG.NUM_DEVICES > 1:\n",
    "    efficientnet_model = nn.DataParallel(efficientnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
       "=================================================================================================================================================\n",
       "EfficientNetV2Model (EfficientNetV2Model)                         [32, 3, 256, 256]    [32, 8]              --                   Partial\n",
       "├─EfficientNet (backbone_model)                                   [32, 3, 256, 256]    [32, 8]              --                   Partial\n",
       "│    └─Sequential (features)                                      [32, 3, 256, 256]    [32, 1280, 8, 8]     --                   False\n",
       "│    │    └─Conv2dNormActivation (0)                              [32, 3, 256, 256]    [32, 32, 128, 128]   (928)                False\n",
       "│    │    └─Sequential (1)                                        [32, 32, 128, 128]   [32, 32, 128, 128]   (37,120)             False\n",
       "│    │    └─Sequential (2)                                        [32, 32, 128, 128]   [32, 64, 64, 64]     (1,032,320)          False\n",
       "│    │    └─Sequential (3)                                        [32, 64, 64, 64]     [32, 96, 32, 32]     (2,390,336)          False\n",
       "│    │    └─Sequential (4)                                        [32, 96, 32, 32]     [32, 192, 16, 16]    (3,553,224)          False\n",
       "│    │    └─Sequential (5)                                        [32, 192, 16, 16]    [32, 224, 16, 16]    (14,501,728)         False\n",
       "│    │    └─Sequential (6)                                        [32, 224, 16, 16]    [32, 384, 8, 8]      (54,866,360)         False\n",
       "│    │    └─Sequential (7)                                        [32, 384, 8, 8]      [32, 640, 8, 8]      (40,030,496)         False\n",
       "│    │    └─Conv2dNormActivation (8)                              [32, 640, 8, 8]      [32, 1280, 8, 8]     (821,760)            False\n",
       "│    └─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 8, 8]     [32, 1280, 1, 1]     --                   --\n",
       "│    └─Sequential (classifier)                                    [32, 1280]           [32, 8]              --                   True\n",
       "│    │    └─Flatten (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    │    └─Dropout (1)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    │    └─Linear (2)                                            [32, 1280]           [32, 256]            327,936              True\n",
       "│    │    └─GELU (3)                                              [32, 256]            [32, 256]            --                   --\n",
       "│    │    └─Dropout (4)                                           [32, 256]            [32, 256]            --                   --\n",
       "│    │    └─Linear (5)                                            [32, 256]            [32, 8]              2,048                True\n",
       "=================================================================================================================================================\n",
       "Total params: 117,564,256\n",
       "Trainable params: 329,984\n",
       "Non-trainable params: 117,234,272\n",
       "Total mult-adds (G): 510.97\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 25.17\n",
       "Forward/backward pass size (MB): 23021.75\n",
       "Params size (MB): 470.26\n",
       "Estimated Total Size (MB): 23517.17\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View model summary\n",
    "summary(\n",
    "    model=efficientnet_model, \n",
    "    input_size=(CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type (var_name))                                                               Input Shape          Output Shape         Param #              Trainable\n",
       "=====================================================================================================================================================================\n",
       "Swinv2Model (Swinv2Model)                                                             [32, 3, 256, 256]    [32, 1536]           --                   True\n",
       "├─Swinv2Embeddings (embeddings)                                                       [32, 3, 256, 256]    [32, 4096, 192]      --                   True\n",
       "│    └─Swinv2PatchEmbeddings (patch_embeddings)                                       [32, 3, 256, 256]    [32, 4096, 192]      --                   True\n",
       "│    │    └─Conv2d (projection)                                                       [32, 3, 256, 256]    [32, 192, 64, 64]    9,408                True\n",
       "│    └─LayerNorm (norm)                                                               [32, 4096, 192]      [32, 4096, 192]      384                  True\n",
       "│    └─Dropout (dropout)                                                              [32, 4096, 192]      [32, 4096, 192]      --                   --\n",
       "├─Swinv2Encoder (encoder)                                                             [32, 4096, 192]      [32, 64, 1536]       --                   True\n",
       "│    └─ModuleList (layers)                                                            --                   --                   --                   True\n",
       "│    │    └─Swinv2Stage (0)                                                           [32, 4096, 192]      [32, 1024, 384]      1,194,252            True\n",
       "│    │    └─Swinv2Stage (1)                                                           [32, 1024, 384]      [32, 256, 768]       4,744,728            True\n",
       "│    │    └─Swinv2Stage (2)                                                           [32, 256, 768]       [32, 64, 1536]       132,538,800          True\n",
       "│    │    └─Swinv2Stage (3)                                                           [32, 64, 1536]       [32, 64, 1536]       56,712,288           True\n",
       "├─LayerNorm (layernorm)                                                               [32, 64, 1536]       [32, 64, 1536]       3,072                True\n",
       "├─AdaptiveAvgPool1d (pooler)                                                          [32, 1536, 64]       [32, 1536, 1]        --                   --\n",
       "=====================================================================================================================================================================\n",
       "Total params: 195,202,932\n",
       "Trainable params: 195,202,932\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.72\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 25.17\n",
       "Forward/backward pass size (MB): 18035.44\n",
       "Params size (MB): 780.81\n",
       "Estimated Total Size (MB): 18841.42\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View model summary\n",
    "summary(\n",
    "    model=Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\"), \n",
    "    input_size=(CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:616: UserWarning: Checkpoint directory ./checkpoints/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | CustomModel | 195 M \n",
      "--------------------------------------\n",
      "195 M     Trainable params\n",
      "0         Non-trainable params\n",
      "195 M     Total params\n",
      "780.812   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tanh(): argument 'input' (position 1) must be Tensor, not Swinv2ModelOutput",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m earlystopping_callback \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback, earlystopping_callback], val_check_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, logger\u001b[38;5;241m=\u001b[39mwandb_logger)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m     35\u001b[0m lit_model\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    731\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    733\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    734\u001b[0m )\n\u001b[1;32m--> 735\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1166\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1168\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1274\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1343\u001b[0m     \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    154\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m--> 155\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:143\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:240\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    the outputs of the step\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 240\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1704\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1704\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1706\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1707\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:370\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[39], line 44\u001b[0m, in \u001b[0;36mLitCustomModel.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 44\u001b[0m probs, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step_output\u001b[38;5;241m.\u001b[39mappend([probs,label])\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 17\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[1;34m(self, x, label)\u001b[0m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# pooler_output 대신에 last_hidden_state 사용\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#x = outputs.last_hidden_state[:, 0]  # [CLS] 토큰에 해당하는 벡터 추출\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\activation.py:357\u001b[0m, in \u001b[0;36mTanh.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: tanh(): argument 'input' (position 1) must be Tensor, not Swinv2ModelOutput"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(train_df, train_df['class'])):\n",
    "    train_fold_df = train_df.loc[train_index,:]\n",
    "    val_fold_df = train_df.loc[val_index,:]\n",
    "\n",
    "    train_dataset = CustomDataset(train_fold_df, 'image_path', mode='train')\n",
    "    val_dataset = CustomDataset(val_fold_df, 'image_path', mode='val')\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = DataLoader(val_dataset, collate_fn=val_collate_fn, batch_size=BATCH_SIZE*2)\n",
    "\n",
    "    #model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "    model = efficientnet_model\n",
    "    lit_model = LitCustomModel(model)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_score',\n",
    "        mode='max',\n",
    "        dirpath='./checkpoints/',\n",
    "        filename=f'swinv2-large-resize-fold_idx={fold_idx}'+'-{epoch:02d}-{train_loss:.4f}-{val_score:.4f}',\n",
    "        #filename=f'EfficientNetV2Model={fold_idx}'+'-{epoch:02d}-{train_loss:.4f}-{val_score:.4f}',\n",
    "        save_top_k=1,\n",
    "        save_weights_only=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # wandb logger 초기화\n",
    "    wandb_logger = WandbLogger(name=f\"swinv2_Fold{fold_idx}\", project=\"Bird_Competition\", log_model=\"all\")\n",
    "    #wandb_logger = WandbLogger(name=f\"EfficientNetV2Model_Fold{fold_idx}\", project=\"Bird_Competition\", log_model=\"all\")\n",
    "\n",
    "    earlystopping_callback = EarlyStopping(monitor=\"val_score\", mode=\"max\", patience=3)\n",
    "    trainer = L.Trainer(max_epochs=100, accelerator='auto', precision=32, callbacks=[checkpoint_callback, earlystopping_callback], val_check_interval=0.5, logger=wandb_logger)\n",
    "    trainer.fit(lit_model, train_dataloader, val_dataloader)\n",
    "\n",
    "    model.cpu()\n",
    "    lit_model.cpu()\n",
    "    del model, lit_model, checkpoint_callback, earlystopping_callback, trainer\n",
    "    #wandb_logger.experiment.finish()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./open/test.csv')\n",
    "test_df['img_path'] = test_df['img_path'].apply(lambda x: os.path.join('./open', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(test_df) == len(os.listdir('./open/test')):\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256,256), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "test_collate_fn = CustomCollateFn(test_transform, 'inference')\n",
    "test_dataset = CustomDataset(test_df, 'img_path', mode='inference')\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=test_collate_fn, batch_size=BATCH_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_preds = []\n",
    "for checkpoint_path in glob('./checkpoints/swinv2-large-resize*.ckpt'):\n",
    "    model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "    lit_model = LitCustomModel.load_from_checkpoint(checkpoint_path, model=model)\n",
    "    trainer = L.Trainer( accelerator='auto', precision=32)\n",
    "    preds = trainer.predict(lit_model, test_dataloader)\n",
    "    preds = torch.cat(preds,dim=0).detach().cpu().numpy().argmax(1)\n",
    "    fold_preds.append(preds)\n",
    "pred_ensemble = list(map(lambda x: np.bincount(x).argmax(),np.stack(fold_preds,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./open/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'] = le.inverse_transform(pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/swinv2_large_resize.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
