{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/dacon/lowresol/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -qn open.zip -d ./open/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet timm pytorch_lightning==1.7.7 torchmetrics==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as L\n",
    "from torch import optim\n",
    "\n",
    "from torchinfo import summary\n",
    "#from glob import glob\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as  transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Swinv2Config, Swinv2Model, AutoImageProcessor, AutoModelForImageClassification\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger  # wandb logger를 임포트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_DEVICES = torch.cuda.device_count()\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "    #NUM_CLASSES = 4\n",
    "    NUM_CLASSES = 8\n",
    "    EPOCHS = 16\n",
    "    # BATCH_SIZE = (\n",
    "    #     32 if torch.cuda.device_count() < 2 \n",
    "    #     else (32 * torch.cuda.device_count())\n",
    "    # )\n",
    "    BATCH_SIZE = 16\n",
    "    LR = 0.001\n",
    "    APPLY_SHUFFLE = True\n",
    "    SEED = 768\n",
    "    #HEIGHT = 224\n",
    "    #WIDTH = 224\n",
    "    HEIGHT = 224\n",
    "    WIDTH = 224\n",
    "    CHANNELS = 3\n",
    "    #IMAGE_SIZE = (224, 224, 3)\n",
    "    IMAGE_SIZE = (224, 224, 3)\n",
    "    \n",
    "    # Define paths\n",
    "    #DATASET_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
    "    #TRAIN_PATH = '/content/drive/MyDrive/Colab Notebooks/dataset/train/'\n",
    "    #TEST_PATH = '/content/drive/MyDrive/Colab Notebooks/dataset/test'\n",
    "    \n",
    "    # Define paths\n",
    "    # DATASET_PATH = \"C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Data\"\n",
    "    # TRAIN_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Data\\\\train\\\\'\n",
    "    # TEST_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Data\\\\test\\\\'\n",
    "\n",
    "    # Define paths\n",
    "    DATASET_PATH = \"C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Datas_3000_3000\"\n",
    "    TRAIN_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Datas_3000_3000\\\\'\n",
    "    TEST_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Datas_3000_2250\\\\'\n",
    "    \n",
    "    \n",
    "# Mute warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'saved'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "result_dir = 'result'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### Get image paths with glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#print(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "train_images = glob.glob(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "valid_images = glob.glob(f\"{CFG.TEST_PATH}**/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### Create Pandas DataFrames for paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(image_paths):\n",
    "    return [(_.split('\\\\')[-2:][0]).replace('-', '_') for _ in image_paths]\n",
    "\n",
    "\n",
    "def build_df(image_paths, labels):\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'image_path': image_paths,\n",
    "        'label': generate_labels(labels)\n",
    "    })\n",
    "    \n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path     label\n",
       "0  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "1  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "2  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "3  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "4  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_kfold_df = build_df(train_images, train_images)\n",
    "valid_no_kfold_df = build_df(valid_images, valid_images)\n",
    "\n",
    "# View first 5 samples in the dataset\n",
    "train_no_kfold_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path     label\n",
       "0  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "1  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "2  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "3  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "4  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_no_kfold_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2424, 2), (2424, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_kfold_df.shape, valid_no_kfold_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### Load & View Random Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load(image_path, as_tensor=True):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if as_tensor:\n",
    "        converter = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            #transforms.Grayscale()\n",
    "        ])\n",
    "        return converter(image)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "\n",
    "def view_sample(image, label, color_map='rgb', fig_size=(8, 10)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    \n",
    "    if color_map=='rgb':\n",
    "        plt.imshow(image)\n",
    "    else:\n",
    "        plt.imshow(image, cmap=color_map)\n",
    "    \n",
    "    plt.title(f'Label: {label}', fontsize=16)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.5'></a>\n",
    "### Create Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, path_col,  mode='train'):\n",
    "        self.df = df\n",
    "        self.path_col = path_col\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            label = row['class']\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'label':label\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'val':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            label = row['class']\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'label':label\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'inference':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            data = {\n",
    "                'image':image,\n",
    "            }\n",
    "            return data\n",
    "\n",
    "    def train_transform(self, image):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollateFn:\n",
    "    def __init__(self, transform, mode):\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.mode=='train':\n",
    "            images = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            labels = torch.LongTensor([data['label'] for data in batch])\n",
    "            return images, labels\n",
    "        elif self.mode=='val':\n",
    "            images = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            labels = torch.LongTensor([data['label'] for data in batch])\n",
    "            return images, labels\n",
    "        elif self.mode=='inference':\n",
    "            images = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            return images, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_SPLIT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_class_mapping(df, pickle_file_path):\n",
    "    # 미리 정의된 클래스 매핑\n",
    "    class_mapping = {\n",
    "        'Bonobono': 0,\n",
    "        'Eagle': 1,\n",
    "        'Gmarket': 2,\n",
    "        'Hp': 3,\n",
    "        'Intel': 4,\n",
    "        'underwood_statue': 5,\n",
    "        'Wonju': 6,\n",
    "        'Yonsei': 7\n",
    "    }\n",
    "    \n",
    "    # pickle 파일이 존재하지 않으면 생성\n",
    "    if not os.path.exists(pickle_file_path):\n",
    "        print(\"Pickle 파일을 생성합니다.\")\n",
    "        df['class'] = df['label'].map(class_mapping)\n",
    "        \n",
    "        with open(pickle_file_path, 'wb') as f:\n",
    "            pickle.dump(class_mapping, f)\n",
    "        \n",
    "        return class_mapping\n",
    "    else:\n",
    "        # pickle 파일이 존재하면 로드\n",
    "        print(\"Pickle 파일을 로드합니다.\")\n",
    "        with open(pickle_file_path, 'rb') as f:\n",
    "            class_mapping = pickle.load(f)\n",
    "        \n",
    "        return class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle 파일을 로드합니다.\n"
     ]
    }
   ],
   "source": [
    "# 함수를 호출하여 class_mapping 생성 또는 로드\n",
    "pickle_file_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\class8.pickle'\n",
    "class_mapping = create_or_load_class_mapping(train_no_kfold_df, pickle_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 매핑을 이용하여 라벨을 클래스 번호로 변환 (예시로 train_no_kfold_df 사용)\n",
    "train_no_kfold_df['class'] = train_no_kfold_df['label'].apply(lambda x: class_mapping.get(x))\n",
    "valid_no_kfold_df['class'] = valid_no_kfold_df['label'].apply(lambda x: class_mapping.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 예시 데이터프레임 생성\n",
    "# valid_no_kfold_df = pd.DataFrame({\n",
    "#     'label': [...],\n",
    "#     'class': [...]\n",
    "# })\n",
    "\n",
    "# 샘플링할 데이터 비율 (예: 전체 데이터의 20%)\n",
    "sample_ratio = 0.4\n",
    "\n",
    "# StratifiedShuffleSplit 객체 생성\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=sample_ratio, random_state=42)\n",
    "\n",
    "# 샘플링\n",
    "for train_index, sample_index in splitter.split(valid_no_kfold_df, valid_no_kfold_df['class']):\n",
    "    sampled_df = valid_no_kfold_df.iloc[sample_index]\n",
    "\n",
    "valid_no_kfold_df = sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not len(train_df) == len(os.listdir('./open/train')):\n",
    "#    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as  transforms\n",
    "from torchvision.transforms import RandomAffine, RandomHorizontalFlip, RandomVerticalFlip, ColorJitter\n",
    "\n",
    "def add_random_noise(image):\n",
    "    noise = torch.randn_like(image) * 0.1\n",
    "    return image + noise\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(CFG.WIDTH,CFG.WIDTH), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    RandomAffine(degrees=30, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random'), \n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Lambda(add_random_noise),\n",
    "    # transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(CFG.WIDTH,CFG.WIDTH), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    RandomAffine(degrees=30, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Lambda(add_random_noise),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "train_collate_fn = CustomCollateFn(train_transform, 'train')\n",
    "val_collate_fn = CustomCollateFn(val_transform, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type (var_name))                                                               Input Shape          Output Shape         Param #              Trainable\n",
       "=====================================================================================================================================================================\n",
       "Swinv2Model (Swinv2Model)                                                             [16, 3, 224, 224]    [16, 1536]           --                   True\n",
       "├─Swinv2Embeddings (embeddings)                                                       [16, 3, 224, 224]    [16, 3136, 192]      --                   True\n",
       "│    └─Swinv2PatchEmbeddings (patch_embeddings)                                       [16, 3, 224, 224]    [16, 3136, 192]      --                   True\n",
       "│    │    └─Conv2d (projection)                                                       [16, 3, 224, 224]    [16, 192, 56, 56]    9,408                True\n",
       "│    └─LayerNorm (norm)                                                               [16, 3136, 192]      [16, 3136, 192]      384                  True\n",
       "│    └─Dropout (dropout)                                                              [16, 3136, 192]      [16, 3136, 192]      --                   --\n",
       "├─Swinv2Encoder (encoder)                                                             [16, 3136, 192]      [16, 49, 1536]       --                   True\n",
       "│    └─ModuleList (layers)                                                            --                   --                   --                   True\n",
       "│    │    └─Swinv2Stage (0)                                                           [16, 3136, 192]      [16, 784, 384]       1,194,252            True\n",
       "│    │    └─Swinv2Stage (1)                                                           [16, 784, 384]       [16, 196, 768]       4,744,728            True\n",
       "│    │    └─Swinv2Stage (2)                                                           [16, 196, 768]       [16, 49, 1536]       132,538,800          True\n",
       "│    │    └─Swinv2Stage (3)                                                           [16, 49, 1536]       [16, 49, 1536]       56,712,288           True\n",
       "├─LayerNorm (layernorm)                                                               [16, 49, 1536]       [16, 49, 1536]       3,072                True\n",
       "├─AdaptiveAvgPool1d (pooler)                                                          [16, 1536, 49]       [16, 1536, 1]        --                   --\n",
       "=====================================================================================================================================================================\n",
       "Total params: 195,202,932\n",
       "Trainable params: 195,202,932\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.72\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 9.63\n",
       "Forward/backward pass size (MB): 7692.49\n",
       "Params size (MB): 780.81\n",
       "Estimated Total Size (MB): 8482.93\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View model summary\n",
    "summary(\n",
    "    model=Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\"), \n",
    "    input_size=(CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pytorch_CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pytorch_CustomModel, self).__init__()\n",
    "        self.model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.LazyLinear(CFG.NUM_CLASSES),\n",
    "        )\n",
    "\n",
    "#     @torch.compile\n",
    "    def forward(self, x, label=None):\n",
    "        # original\n",
    "        x = self.model(x).pooler_output\n",
    "        x = self.clf(x)\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x, label)\n",
    "        probs = nn.LogSoftmax(dim=-1)(x)\n",
    "        return probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, total_epoch, train_dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch}/{total_epoch}\")\n",
    "    for i, (data, target) in progress_bar:\n",
    "        data, target = data.to(CFG.DEVICE), target.to(CFG.DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output, loss_ = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "         # 손실 및 정확도 계산\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        # tqdm의 설명 부분을 업데이트\n",
    "        progress_bar.set_postfix(loss=running_loss/(i+1), accuracy=100.*correct/total)\n",
    "    # 에포크 완료 후 출력\n",
    "    print(f\"Epoch {epoch}, Loss: {running_loss/len(train_dataloader):.4f}, Accuracy: {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수\n",
    "def valid(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    progress_bar = tqdm(val_loader, total=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for data, target in progress_bar:\n",
    "            data, target = data.to(CFG.DEVICE), target.to(CFG.DEVICE)\n",
    "            output, loss_ = model(data)\n",
    "            val_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print('Valid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    #wandb.log({\"Valid Accuracy\": 100. * correct / len(val_loader.dataset), \"Valid Loss\": val_loss})\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "Epoch 0/16:   0%|          | 0/152 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CFG\u001b[38;5;241m.\u001b[39mEPOCHS):\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m valid(model, val_dataloader, criterion)\n\u001b[0;32m     27\u001b[0m     checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[25], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, epoch, total_epoch, train_dataloader, criterion, optimizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(CFG\u001b[38;5;241m.\u001b[39mDEVICE), target\u001b[38;5;241m.\u001b[39mto(CFG\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m output, loss_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 13\u001b[0m, in \u001b[0;36mPytorch_CustomModel.forward\u001b[1;34m(self, x, label)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# original\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclf(x)\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:1058\u001b[0m, in \u001b[0;36mSwinv2Model.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1054\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdepths))\n\u001b[0;32m   1056\u001b[0m embedding_output, input_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[1;32m-> 1058\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1068\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:879\u001b[0m, in \u001b[0;36mSwinv2Encoder.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, return_dict)\u001b[0m\n\u001b[0;32m    875\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    876\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, input_dimensions, layer_head_mask\n\u001b[0;32m    877\u001b[0m     )\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    886\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    887\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:799\u001b[0m, in \u001b[0;36mSwinv2Stage.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m    797\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    808\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:729\u001b[0m, in \u001b[0;36mSwinv2Layer.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    727\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39mto(hidden_states_windows\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 729\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    735\u001b[0m attention_windows \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, channels)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:602\u001b[0m, in \u001b[0;36mSwinv2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    597\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    601\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 602\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:504\u001b[0m, in \u001b[0;36mSwinv2SelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# cosine attention\u001b[39;00m\n\u001b[0;32m    501\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(query_layer, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(\n\u001b[0;32m    502\u001b[0m     key_layer, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    503\u001b[0m )\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 504\u001b[0m logit_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogit_scale\u001b[49m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0.01\u001b[39m))\u001b[38;5;241m.\u001b[39mexp()\n\u001b[0;32m    505\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m*\u001b[39m logit_scale\n\u001b[0;32m    506\u001b[0m relative_position_bias_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_position_bias_mlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_coords_table)\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads\n\u001b[0;32m    508\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_fold_df = train_no_kfold_df\n",
    "val_fold_df = valid_no_kfold_df\n",
    "\n",
    "train_dataset = CustomDataset(train_fold_df, 'image_path', mode='train')\n",
    "val_dataset = CustomDataset(val_fold_df, 'image_path', mode='val')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=CFG.BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=val_collate_fn, batch_size=CFG.BATCH_SIZE)\n",
    "\n",
    "# 모델, 손실함수, 최적화함수 설정\n",
    "model = Pytorch_CustomModel().to(CFG.DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-5, weight_decay=5e-4, eps=5e-9)\n",
    "\n",
    "# wandb에 모델, 최적화 함수 로그\n",
    "#wandb.watch(model, log=\"all\")\n",
    "#wandb.config.update({\"Optimizer\": \"ADAM\", \"Learning Rate\": 0.01, \"Momentum\": 0.5})\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(CFG.EPOCHS):\n",
    "    \n",
    "    train(model, epoch, CFG.EPOCHS, train_dataloader, criterion, optimizer)\n",
    "\n",
    "    val_loss = valid(model, val_dataloader, criterion)\n",
    "    checkpoint_dir = './checkpoints'\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    # 모델 가중치 저장\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'Swinv2Mode_epoch{epoch}_valid_loss{val_loss}.pth')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    # Validation 성능이 향상될 때마다 가장 좋은 모델 가중치 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_checkpoint_path = os.path.join(checkpoint_dir, f'Swinv2Mode_best_model_valid_loss{val_loss}.pth')\n",
    "        torch.save(model.state_dict(), best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:353: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:343: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if width % self.patch_size[1] != 0:\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:346: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height % self.patch_size[0] != 0:\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:744: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  was_padded = pad_values[3] > 0 or pad_values[5] > 0\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:745: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if was_padded:\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:388: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  should_pad = (height % 2 == 1) or (width % 2 == 1)\n",
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:389: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if should_pad:\n"
     ]
    },
    {
     "ename": "SymbolicValueError",
     "evalue": "Unsupported: ONNX export of Pad in opset 9. The sizes of the padding must be constant. Please try opset version 11.  [Caused by the value '594 defined in (%594 : int[] = prim::ListConstruct(%518, %518, %518, %pad_right, %518, %pad_bottom), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n)' (type 'List[int]') in the TorchScript graph. The containing node has kind 'prim::ListConstruct'.] \n\n    Inputs:\n        #0: 518 defined in (%518 : Long(device=cpu) = onnx::Constant[value={0}](), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'Tensor')\n        #1: 518 defined in (%518 : Long(device=cpu) = onnx::Constant[value={0}](), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'Tensor')\n        #2: 518 defined in (%518 : Long(device=cpu) = onnx::Constant[value={0}](), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'Tensor')\n        #3: pad_right defined in (%pad_right : Long(requires_grad=0, device=cpu) = onnx::Sub(%547, %561), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:695:0\n    )  (type 'Tensor')\n        #4: 518 defined in (%518 : Long(device=cpu) = onnx::Constant[value={0}](), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'Tensor')\n        #5: pad_bottom defined in (%pad_bottom : Long(requires_grad=0, device=cpu) = onnx::Sub(%578, %592), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:696:0\n    )  (type 'Tensor')\n    Outputs:\n        #0: 594 defined in (%594 : int[] = prim::ListConstruct(%518, %518, %518, %pad_right, %518, %pad_bottom), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'List[int]')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSymbolicValueError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:1937\u001b[0m, in \u001b[0;36m_convert_padding_node\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1937\u001b[0m     padding \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1938\u001b[0m         symbolic_helper\u001b[38;5;241m.\u001b[39m_get_const(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m input_list\n\u001b[0;32m   1939\u001b[0m     ]\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;66;03m# FIXME(justinchuby): Avoid catching Exception.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m     \u001b[38;5;66;03m# Catch a more specific exception instead.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:1938\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1936\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1937\u001b[0m     padding \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 1938\u001b[0m         \u001b[43msymbolic_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_const\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpadding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m input_list\n\u001b[0;32m   1939\u001b[0m     ]\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;66;03m# FIXME(justinchuby): Avoid catching Exception.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m     \u001b[38;5;66;03m# Catch a more specific exception instead.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:173\u001b[0m, in \u001b[0;36m_get_const\u001b[1;34m(value, desc, arg_name)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_constant(value):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mSymbolicValueError(\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNX symbolic expected a constant value of the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m argument, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    176\u001b[0m         value,\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse_arg(value, desc)\n",
      "\u001b[1;31mSymbolicValueError\u001b[0m: ONNX symbolic expected a constant value of the 'padding' argument, got 'pad_right defined in (%pad_right : Long(requires_grad=0, device=cpu) = onnx::Sub(%547, %561), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:695:0\n)'  [Caused by the value 'pad_right defined in (%pad_right : Long(requires_grad=0, device=cpu) = onnx::Sub(%547, %561), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:695:0\n)' (type 'Tensor') in the TorchScript graph. The containing node has kind 'onnx::Sub'.] \n    (node defined in c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py(695): maybe_pad\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py(714): forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1522): _slow_forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1541): _call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1532): _wrapped_call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py(799): forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1522): _slow_forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1541): _call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1532): _wrapped_call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py(879): forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1522): _slow_forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1541): _call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1532): _wrapped_call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py(1058): forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1522): _slow_forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1541): _call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1532): _wrapped_call_impl\nC:\\Users\\Seo\\AppData\\Local\\Temp\\ipykernel_20320\\2141217592.py(13): forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1522): _slow_forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1541): _call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1532): _wrapped_call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py(129): wrapper\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py(138): forward\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1541): _call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py(1532): _wrapped_call_impl\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py(1310): _get_trace_graph\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py(914): _trace_and_get_graph_from_model\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py(1010): _create_jit_graph\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py(1134): _model_to_graph\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py(1612): _export\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py(516): export\nC:\\Users\\Seo\\AppData\\Local\\Temp\\ipykernel_20320\\3899632504.py(14): <module>\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\IPython\\core\\interactiveshell.py(3577): run_code\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\IPython\\core\\interactiveshell.py(3517): run_ast_nodes\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\IPython\\core\\interactiveshell.py(3334): run_cell_async\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\IPython\\core\\async_helpers.py(129): _pseudo_sync_runner\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\IPython\\core\\interactiveshell.py(3130): _run_cell\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\IPython\\core\\interactiveshell.py(3075): run_cell\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel\\zmqshell.py(549): run_cell\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel\\ipkernel.py(446): do_execute\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel\\kernelbase.py(778): execute_request\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel\\ipkernel.py(359): execute_request\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel\\kernelbase.py(437): dispatch_shell\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel\\kernelbase.py(534): process_one\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel\\kernelbase.py(545): dispatch_queue\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\asyncio\\events.py(80): _run\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\asyncio\\base_events.py(1909): _run_once\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\asyncio\\base_events.py(603): run_forever\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\tornado\\platform\\asyncio.py(215): start\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel\\kernelapp.py(739): start\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\traitlets\\config\\application.py(1075): launch_instance\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\ipykernel_launcher.py(18): <module>\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\runpy.py(86): _run_code\nc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\runpy.py(196): _run_module_as_main\n)\n\n    Inputs:\n        #0: 547 defined in (%547 : Long(requires_grad=0, device=cpu) = onnx::Sub(%531, %546), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\_tensor.py:966:0\n    )  (type 'Tensor')\n        #1: 561 defined in (%561 : Long(device=cpu) = onnx::Mul(%560, %531), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:695:0\n    )  (type 'Tensor')\n    Outputs:\n        #0: pad_right defined in (%pad_right : Long(requires_grad=0, device=cpu) = onnx::Sub(%547, %561), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:695:0\n    )  (type 'Tensor')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSymbolicValueError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (CFG\u001b[38;5;241m.\u001b[39mBATCH_SIZE, CFG\u001b[38;5;241m.\u001b[39mCHANNELS, CFG\u001b[38;5;241m.\u001b[39mWIDTH, CFG\u001b[38;5;241m.\u001b[39mHEIGHT)\n\u001b[0;32m     12\u001b[0m example_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(input_shape)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[0;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1612\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m   1609\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1610\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1612\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1627\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1138\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1135\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1138\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43m_optimize_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_disable_torch_constant_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1149\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch IR graph at exception: \u001b[39m\u001b[38;5;124m\"\u001b[39m, graph)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:677\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[1;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[0;32m    674\u001b[0m     _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[0;32m    675\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m--> 677\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_pass_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m    679\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1956\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[1;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[0;32m   1951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m symbolic_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1952\u001b[0m         \u001b[38;5;66;03m# TODO Wrap almost identical attrs assignment or comment the difference.\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m         attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1954\u001b[0m             k: symbolic_helper\u001b[38;5;241m.\u001b[39m_node_get(node, k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattributeNames()\n\u001b[0;32m   1955\u001b[0m         }\n\u001b[1;32m-> 1956\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m symbolic_fn(graph_context, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattrs)\n\u001b[0;32m   1958\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1959\u001b[0m     k \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39mkindOf(k)[\u001b[38;5;241m0\u001b[39m]: symbolic_helper\u001b[38;5;241m.\u001b[39m_node_get(node, k)\n\u001b[0;32m   1960\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mattributeNames()\n\u001b[0;32m   1961\u001b[0m }\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1963\u001b[0m     \u001b[38;5;66;03m# Clone node to trigger ONNX shape inference\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:2052\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(g, input, pad, mode, value)\u001b[0m\n\u001b[0;32m   2050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reflection_pad(g, \u001b[38;5;28minput\u001b[39m, pad)\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_pad_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcircular\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2054\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pad_circular(g, \u001b[38;5;28minput\u001b[39m, pad)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:1962\u001b[0m, in \u001b[0;36mconstant_pad_nd\u001b[1;34m(g, input, padding, value)\u001b[0m\n\u001b[0;32m   1955\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1956\u001b[0m     \u001b[38;5;66;03m# FIXME(justinchuby): Avoid catching Exception.\u001b[39;00m\n\u001b[0;32m   1957\u001b[0m     \u001b[38;5;66;03m# Catch a more specific exception instead.\u001b[39;00m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m symbolic_helper\u001b[38;5;241m.\u001b[39m_onnx_opset_unsupported_detailed(\n\u001b[0;32m   1959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe value for the padding must be constant\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\n\u001b[0;32m   1960\u001b[0m     )\n\u001b[1;32m-> 1962\u001b[0m padding \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_padding_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m paddings \u001b[38;5;241m=\u001b[39m _prepare_onnx_paddings(symbolic_helper\u001b[38;5;241m.\u001b[39m_get_tensor_rank(\u001b[38;5;28minput\u001b[39m), padding)\n\u001b[0;32m   1964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _op_with_optional_float_cast(\n\u001b[0;32m   1965\u001b[0m     g, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPad\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m, pads_i\u001b[38;5;241m=\u001b[39mpaddings, mode_s\u001b[38;5;241m=\u001b[39mmode, value_f\u001b[38;5;241m=\u001b[39mvalue, opset_before\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m\n\u001b[0;32m   1966\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:1943\u001b[0m, in \u001b[0;36m_convert_padding_node\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   1937\u001b[0m         padding \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1938\u001b[0m             symbolic_helper\u001b[38;5;241m.\u001b[39m_get_const(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m input_list\n\u001b[0;32m   1939\u001b[0m         ]\n\u001b[0;32m   1940\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1941\u001b[0m         \u001b[38;5;66;03m# FIXME(justinchuby): Avoid catching Exception.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m         \u001b[38;5;66;03m# Catch a more specific exception instead.\u001b[39;00m\n\u001b[1;32m-> 1943\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msymbolic_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_onnx_opset_unsupported_detailed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1944\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe sizes of the padding must be constant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\n\u001b[0;32m   1945\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m padding\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:662\u001b[0m, in \u001b[0;36m_onnx_opset_unsupported_detailed\u001b[1;34m(op_name, current_opset, supported_opset, reason, value)\u001b[0m\n\u001b[0;32m    657\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported: ONNX export of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_opset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please try opset version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_opset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m )\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, _C\u001b[38;5;241m.\u001b[39mValue):\n\u001b[1;32m--> 662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mSymbolicValueError(\n\u001b[0;32m    663\u001b[0m         message,\n\u001b[0;32m    664\u001b[0m         value,\n\u001b[0;32m    665\u001b[0m     )\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOnnxExporterError(message)\n",
      "\u001b[1;31mSymbolicValueError\u001b[0m: Unsupported: ONNX export of Pad in opset 9. The sizes of the padding must be constant. Please try opset version 11.  [Caused by the value '594 defined in (%594 : int[] = prim::ListConstruct(%518, %518, %518, %pad_right, %518, %pad_bottom), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n)' (type 'List[int]') in the TorchScript graph. The containing node has kind 'prim::ListConstruct'.] \n\n    Inputs:\n        #0: 518 defined in (%518 : Long(device=cpu) = onnx::Constant[value={0}](), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'Tensor')\n        #1: 518 defined in (%518 : Long(device=cpu) = onnx::Constant[value={0}](), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'Tensor')\n        #2: 518 defined in (%518 : Long(device=cpu) = onnx::Constant[value={0}](), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'Tensor')\n        #3: pad_right defined in (%pad_right : Long(requires_grad=0, device=cpu) = onnx::Sub(%547, %561), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:695:0\n    )  (type 'Tensor')\n        #4: 518 defined in (%518 : Long(device=cpu) = onnx::Constant[value={0}](), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'Tensor')\n        #5: pad_bottom defined in (%pad_bottom : Long(requires_grad=0, device=cpu) = onnx::Sub(%578, %592), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0 # c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\transformers\\models\\swinv2\\modeling_swinv2.py:696:0\n    )  (type 'Tensor')\n    Outputs:\n        #0: 594 defined in (%594 : int[] = prim::ListConstruct(%518, %518, %518, %pad_right, %518, %pad_bottom), scope: __main__.Pytorch_CustomModel::/transformers.models.swinv2.modeling_swinv2.Swinv2Model::model/transformers.models.swinv2.modeling_swinv2.Swinv2Encoder::encoder/transformers.models.swinv2.modeling_swinv2.Swinv2Stage::layers.0/transformers.models.swinv2.modeling_swinv2.Swinv2Layer::blocks.0\n    )  (type 'List[int]')"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import torch\n",
    "\n",
    "path_to_pytorch_model = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Swinv2Mode_best_model.pth'\n",
    "model = Pytorch_CustomModel()\n",
    "model.load_state_dict(torch.load(path_to_pytorch_model))\n",
    "model.eval()\n",
    "\n",
    "onnx_model_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Swinv2Mode_best_model.onnx'\n",
    "input_shape = (CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT)\n",
    "\n",
    "example_input = torch.rand(input_shape)\n",
    "\n",
    "torch.onnx.export(model, example_input, onnx_model_path, export_params=True, opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_fold_df = train_no_kfold_df\n",
    "val_fold_df = valid_no_kfold_df\n",
    "\n",
    "train_dataset = CustomDataset(train_fold_df, 'image_path', mode='train')\n",
    "val_dataset = CustomDataset(val_fold_df, 'image_path', mode='val')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=CFG.BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=val_collate_fn, batch_size=CFG.BATCH_SIZE)\n",
    "\n",
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "#model = efficientnet_model\n",
    "\n",
    "lit_model = LitCustomModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type (var_name))                                                               Input Shape          Output Shape         Param #              Trainable\n",
       "=====================================================================================================================================================================\n",
       "Swinv2Model (Swinv2Model)                                                             [32, 3, 224, 224]    [32, 1536]           --                   True\n",
       "├─Swinv2Embeddings (embeddings)                                                       [32, 3, 224, 224]    [32, 3136, 192]      --                   True\n",
       "│    └─Swinv2PatchEmbeddings (patch_embeddings)                                       [32, 3, 224, 224]    [32, 3136, 192]      --                   True\n",
       "│    │    └─Conv2d (projection)                                                       [32, 3, 224, 224]    [32, 192, 56, 56]    9,408                True\n",
       "│    └─LayerNorm (norm)                                                               [32, 3136, 192]      [32, 3136, 192]      384                  True\n",
       "│    └─Dropout (dropout)                                                              [32, 3136, 192]      [32, 3136, 192]      --                   --\n",
       "├─Swinv2Encoder (encoder)                                                             [32, 3136, 192]      [32, 49, 1536]       --                   True\n",
       "│    └─ModuleList (layers)                                                            --                   --                   --                   True\n",
       "│    │    └─Swinv2Stage (0)                                                           [32, 3136, 192]      [32, 784, 384]       1,194,252            True\n",
       "│    │    └─Swinv2Stage (1)                                                           [32, 784, 384]       [32, 196, 768]       4,744,728            True\n",
       "│    │    └─Swinv2Stage (2)                                                           [32, 196, 768]       [32, 49, 1536]       132,538,800          True\n",
       "│    │    └─Swinv2Stage (3)                                                           [32, 49, 1536]       [32, 49, 1536]       56,712,288           True\n",
       "├─LayerNorm (layernorm)                                                               [32, 49, 1536]       [32, 49, 1536]       3,072                True\n",
       "├─AdaptiveAvgPool1d (pooler)                                                          [32, 1536, 49]       [32, 1536, 1]        --                   --\n",
       "=====================================================================================================================================================================\n",
       "Total params: 195,202,932\n",
       "Trainable params: 195,202,932\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.44\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 15292.76\n",
       "Params size (MB): 780.81\n",
       "Estimated Total Size (MB): 16092.84\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View model summary\n",
    "summary(\n",
    "    model=lit_model, \n",
    "    input_size=(CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 파일 경로\n",
    "ckpt_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\checkpoints\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.ckpt'\n",
    "\n",
    "# 체크포인트 로드\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "# 모델 상태 로드\n",
    "lit_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# .pth 파일 경로\n",
    "pth_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.pth'\n",
    "\n",
    "# .pth 파일로 저장\n",
    "torch.save(lit_model.state_dict(), pth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CustomModel.forward() takes from 2 to 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSeo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2024_06_Dron_Competition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mswinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (CFG\u001b[38;5;241m.\u001b[39mBATCH_SIZE, CFG\u001b[38;5;241m.\u001b[39mCHANNELS, CFG\u001b[38;5;241m.\u001b[39mWIDTH, CFG\u001b[38;5;241m.\u001b[39mHEIGHT)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mlit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1896\u001b[0m, in \u001b[0;36mLightningModule.to_onnx\u001b[1;34m(self, file_path, input_sample, **kwargs)\u001b[0m\n\u001b[0;32m   1893\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1894\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(input_sample)\n\u001b[1;32m-> 1896\u001b[0m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;28mself\u001b[39m, input_sample, file_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(mode)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[0;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1612\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m   1609\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1610\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1612\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1627\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1134\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m   1133\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[1;32m-> 1134\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1010\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[0;32m   1006\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     )\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1010\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1011\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m   1012\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:914\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    912\u001b[0m prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[0;32m    913\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 914\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[0;32m    923\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py:1310\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m   1309\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m-> 1310\u001b[0m outs \u001b[38;5;241m=\u001b[39m ONNXTracedModule(\n\u001b[0;32m   1311\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[0;32m   1312\u001b[0m )(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py:138\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[1;32m--> 138\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py:129\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    128\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[1;32m--> 129\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    131\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[1;32mIn[43], line 38\u001b[0m, in \u001b[0;36mLitCustomModel.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[1;31mTypeError\u001b[0m: CustomModel.forward() takes from 2 to 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "lit_model = LitCustomModel(model)\n",
    "# 체크포인트 파일 경로\n",
    "\n",
    "ckpt_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\checkpoints\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.ckpt'\n",
    "\n",
    "# 체크포인트 로드\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "# 모델 상태 로드\n",
    "lit_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "lit_model.eval()\n",
    "\n",
    "onnx_model_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.onnx'\n",
    "\n",
    "input_shape = (CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT)\n",
    "\n",
    "lit_model.to_onnx(onnx_model_path, input_shape, export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m path_to_pytorch_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSeo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2024_06_Dron_Competition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mswinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m lit_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path_to_pytorch_model)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mlit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[0;32m      8\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSeo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2024_06_Dron_Competition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mswinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (CFG\u001b[38;5;241m.\u001b[39mBATCH_SIZE, CFG\u001b[38;5;241m.\u001b[39mCHANNELS, CFG\u001b[38;5;241m.\u001b[39mWIDTH, CFG\u001b[38;5;241m.\u001b[39mHEIGHT)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "lit_model = LitCustomModel(model)\n",
    "path_to_pytorch_model = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.pth'\n",
    "lit_model = torch.load(path_to_pytorch_model)\n",
    "lit_model.eval()\n",
    "\n",
    "onnx_model_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.onnx'\n",
    "input_shape = (CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT)\n",
    "\n",
    "example_input = torch.rand(input_shape)\n",
    "\n",
    "torch.onnx.export(lit_model, example_input, onnx_model_path, export_params=True, opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./open/test.csv')\n",
    "test_df['img_path'] = test_df['img_path'].apply(lambda x: os.path.join('./open', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(test_df) == len(os.listdir('./open/test')):\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256,256), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "test_collate_fn = CustomCollateFn(test_transform, 'inference')\n",
    "test_dataset = CustomDataset(test_df, 'img_path', mode='inference')\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=test_collate_fn, batch_size=CFG.BATCH_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_preds = []\n",
    "for checkpoint_path in glob('./checkpoints/swinv2-large-resize*.ckpt'):\n",
    "    model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "    lit_model = LitCustomModel.load_from_checkpoint(checkpoint_path, model=model)\n",
    "    trainer = L.Trainer( accelerator='auto', precision=32)\n",
    "    preds = trainer.predict(lit_model, test_dataloader)\n",
    "    preds = torch.cat(preds,dim=0).detach().cpu().numpy().argmax(1)\n",
    "    fold_preds.append(preds)\n",
    "pred_ensemble = list(map(lambda x: np.bincount(x).argmax(),np.stack(fold_preds,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./open/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'] = le.inverse_transform(pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/swinv2_large_resize.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
